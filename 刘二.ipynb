{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f20f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6caca1cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# w.shape\u001b[39;00m\n\u001b[0;32m      5\u001b[0m Y\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[0.],[0.],[0.]], requires_grad=True)\n",
    "tmp = (y-torch.mm(x,w))\n",
    "Y = torch.mean(1/2*torch.mm(tmp.T, tmp))\n",
    "# w.shape\n",
    "Y.backward()\n",
    "Y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "517c53b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-20400., -24322.,  -1836.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = -1*torch.matmul(x.T, y)\n",
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b26a3e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m w\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "Y.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d62ce488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1036.1953125\n",
      "10 1.0189915895462036\n",
      "20 0.8569296002388\n",
      "30 0.7277219295501709\n",
      "40 0.6245462894439697\n",
      "50 0.5420341491699219\n",
      "60 0.47595176100730896\n",
      "70 0.4229546785354614\n",
      "80 0.3803957998752594\n",
      "90 0.34617653489112854\n",
      "100 0.3186298906803131\n",
      "110 0.296429842710495\n",
      "120 0.2785196006298065\n",
      "130 0.2640557289123535\n",
      "140 0.25236406922340393\n",
      "150 0.2429049164056778\n",
      "160 0.23524561524391174\n",
      "170 0.22903886437416077\n",
      "180 0.2240055501461029\n",
      "190 0.2199210673570633\n"
     ]
    }
   ],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(3, 1, bias=True )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = LinearModel()\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.1)\n",
    "\n",
    "for epoch in range(200):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "# print('b = ', model.linea.bias.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "585be26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7591],\n",
       "        [1.6856],\n",
       "        [1.7958],\n",
       "        [1.5700],\n",
       "        [1.7628],\n",
       "        [1.4707],\n",
       "        [1.5718],\n",
       "        [1.5193],\n",
       "        [1.7211],\n",
       "        [1.3823],\n",
       "        [1.5491],\n",
       "        [1.5513],\n",
       "        [1.7613],\n",
       "        [1.5525],\n",
       "        [1.6215],\n",
       "        [1.6331],\n",
       "        [1.5106],\n",
       "        [1.8135],\n",
       "        [1.4949],\n",
       "        [1.5947],\n",
       "        [1.4283],\n",
       "        [1.4409],\n",
       "        [1.4430],\n",
       "        [1.5947],\n",
       "        [1.7025],\n",
       "        [1.4355],\n",
       "        [1.4756],\n",
       "        [1.4756],\n",
       "        [1.5318],\n",
       "        [1.6053],\n",
       "        [1.6753],\n",
       "        [1.6373],\n",
       "        [1.4722],\n",
       "        [1.6913],\n",
       "        [1.6561],\n",
       "        [1.5562],\n",
       "        [1.6893],\n",
       "        [1.3899],\n",
       "        [1.4162],\n",
       "        [1.5424],\n",
       "        [1.3637],\n",
       "        [1.3706],\n",
       "        [1.6036],\n",
       "        [1.3385],\n",
       "        [1.5511],\n",
       "        [1.6509],\n",
       "        [1.3514],\n",
       "        [1.3571],\n",
       "        [1.3847],\n",
       "        [1.5511],\n",
       "        [1.6898],\n",
       "        [1.5843],\n",
       "        [1.5843],\n",
       "        [1.6948],\n",
       "        [1.3655],\n",
       "        [1.3322],\n",
       "        [1.3953],\n",
       "        [1.3586],\n",
       "        [1.4182],\n",
       "        [1.3962],\n",
       "        [1.5651],\n",
       "        [1.4951],\n",
       "        [1.1333],\n",
       "        [1.3393],\n",
       "        [1.5126],\n",
       "        [1.5126],\n",
       "        [1.4310],\n",
       "        [1.5057],\n",
       "        [1.5022],\n",
       "        [1.3795],\n",
       "        [1.4724],\n",
       "        [1.5459],\n",
       "        [1.5653],\n",
       "        [1.4727],\n",
       "        [1.2627],\n",
       "        [1.3945],\n",
       "        [1.3936],\n",
       "        [1.4567],\n",
       "        [1.3936],\n",
       "        [1.5379],\n",
       "        [1.4867],\n",
       "        [1.5407],\n",
       "        [1.5705],\n",
       "        [1.3411],\n",
       "        [1.5740],\n",
       "        [1.2594],\n",
       "        [1.4742],\n",
       "        [1.6073],\n",
       "        [1.5705],\n",
       "        [1.2484],\n",
       "        [1.6109],\n",
       "        [1.3711],\n",
       "        [1.4191],\n",
       "        [1.3884],\n",
       "        [1.2449],\n",
       "        [1.4217],\n",
       "        [1.3230],\n",
       "        [1.3692],\n",
       "        [1.4690],\n",
       "        [1.3359],\n",
       "        [1.3817],\n",
       "        [1.5023],\n",
       "        [1.5355],\n",
       "        [1.2257],\n",
       "        [1.2888],\n",
       "        [1.5550],\n",
       "        [1.4608],\n",
       "        [1.1788],\n",
       "        [1.1788],\n",
       "        [1.4921],\n",
       "        [1.2890],\n",
       "        [1.3430],\n",
       "        [1.4165],\n",
       "        [1.4830],\n",
       "        [1.3640],\n",
       "        [1.3627],\n",
       "        [1.2940],\n",
       "        [1.3307],\n",
       "        [1.4638],\n",
       "        [1.2607],\n",
       "        [1.3273],\n",
       "        [1.3536],\n",
       "        [1.2975],\n",
       "        [1.4936],\n",
       "        [1.2999],\n",
       "        [1.3781],\n",
       "        [1.2117],\n",
       "        [1.2575],\n",
       "        [1.2782],\n",
       "        [1.3413],\n",
       "        [1.1750],\n",
       "        [1.2048],\n",
       "        [1.2782],\n",
       "        [1.3781],\n",
       "        [1.3975],\n",
       "        [1.4411],\n",
       "        [1.1475],\n",
       "        [1.1821],\n",
       "        [1.3346],\n",
       "        [1.1890],\n",
       "        [1.3921],\n",
       "        [1.4219],\n",
       "        [1.4956],\n",
       "        [1.1663],\n",
       "        [1.2627],\n",
       "        [1.3923],\n",
       "        [1.2398],\n",
       "        [1.2925],\n",
       "        [1.5060],\n",
       "        [1.1927],\n",
       "        [1.1892],\n",
       "        [1.2731],\n",
       "        [1.2696],\n",
       "        [1.3396],\n",
       "        [1.3729],\n",
       "        [1.4727],\n",
       "        [1.1069],\n",
       "        [1.3502],\n",
       "        [1.1436],\n",
       "        [1.1562],\n",
       "        [1.2042],\n",
       "        [1.3122],\n",
       "        [1.1173],\n",
       "        [1.1838],\n",
       "        [1.1804],\n",
       "        [1.2171],\n",
       "        [1.2871],\n",
       "        [1.1300],\n",
       "        [1.2549],\n",
       "        [1.3172],\n",
       "        [1.4101],\n",
       "        [1.1106],\n",
       "        [1.1577],\n",
       "        [1.3677],\n",
       "        [1.2420],\n",
       "        [1.2346],\n",
       "        [1.4433],\n",
       "        [1.2022],\n",
       "        [1.0981],\n",
       "        [1.2346],\n",
       "        [1.2279],\n",
       "        [1.3632],\n",
       "        [1.0788],\n",
       "        [1.0823],\n",
       "        [1.0788],\n",
       "        [1.2863],\n",
       "        [1.3485],\n",
       "        [1.4379],\n",
       "        [1.3723],\n",
       "        [1.3783],\n",
       "        [1.2649],\n",
       "        [1.3189],\n",
       "        [1.1296],\n",
       "        [1.3223],\n",
       "        [1.3258],\n",
       "        [1.3625],\n",
       "        [1.1296],\n",
       "        [1.1264],\n",
       "        [1.1286],\n",
       "        [1.2400],\n",
       "        [1.2124],\n",
       "        [1.4099],\n",
       "        [1.1437],\n",
       "        [1.1769],\n",
       "        [1.2102],\n",
       "        [1.2435],\n",
       "        [1.2435],\n",
       "        [1.3433],\n",
       "        [1.0579],\n",
       "        [1.0808],\n",
       "        [1.1210],\n",
       "        [1.3241],\n",
       "        [1.0246],\n",
       "        [1.0143],\n",
       "        [1.1175],\n",
       "        [1.0175],\n",
       "        [1.0719],\n",
       "        [1.1718],\n",
       "        [1.2016],\n",
       "        [1.1912],\n",
       "        [1.2050],\n",
       "        [1.1808],\n",
       "        [1.2945],\n",
       "        [0.9607],\n",
       "        [1.2191],\n",
       "        [1.0825],\n",
       "        [1.2787],\n",
       "        [0.9996],\n",
       "        [1.1858],\n",
       "        [1.0828],\n",
       "        [1.1999],\n",
       "        [1.2962],\n",
       "        [1.0002],\n",
       "        [1.0002],\n",
       "        [1.1666],\n",
       "        [1.1722],\n",
       "        [1.2997],\n",
       "        [1.0002],\n",
       "        [0.9361],\n",
       "        [0.9477],\n",
       "        [1.1150],\n",
       "        [1.0601],\n",
       "        [1.2139],\n",
       "        [1.2139],\n",
       "        [0.9477],\n",
       "        [0.9916],\n",
       "        [1.0283],\n",
       "        [1.0616],\n",
       "        [1.0949],\n",
       "        [1.0949],\n",
       "        [1.1947],\n",
       "        [0.9972],\n",
       "        [0.9316],\n",
       "        [0.9093],\n",
       "        [1.0662],\n",
       "        [1.1755],\n",
       "        [1.0091],\n",
       "        [1.2420],\n",
       "        [0.8901],\n",
       "        [0.9830],\n",
       "        [0.9471],\n",
       "        [1.1044],\n",
       "        [0.8901],\n",
       "        [1.0897],\n",
       "        [1.1895],\n",
       "        [0.9164],\n",
       "        [1.0897],\n",
       "        [1.1861],\n",
       "        [0.8708],\n",
       "        [0.9257],\n",
       "        [0.8708],\n",
       "        [0.8674],\n",
       "        [1.2036],\n",
       "        [1.0236],\n",
       "        [1.0145],\n",
       "        [1.1178],\n",
       "        [1.1178],\n",
       "        [0.9514],\n",
       "        [1.0845],\n",
       "        [1.1318],\n",
       "        [1.1651],\n",
       "        [1.0850],\n",
       "        [0.8797],\n",
       "        [1.0461],\n",
       "        [1.0793],\n",
       "        [0.7939],\n",
       "        [0.7801],\n",
       "        [1.0117],\n",
       "        [1.0934],\n",
       "        [1.1267],\n",
       "        [0.7996],\n",
       "        [0.9603],\n",
       "        [1.1005],\n",
       "        [0.9219],\n",
       "        [0.7555],\n",
       "        [0.9551],\n",
       "        [1.0446],\n",
       "        [0.8694],\n",
       "        [1.0690],\n",
       "        [0.9396],\n",
       "        [0.8834],\n",
       "        [0.8275],\n",
       "        [0.9781],\n",
       "        [0.8819],\n",
       "        [0.8696],\n",
       "        [0.5371]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0423dbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]],  dtype=torch.float32)\n",
    "y_data = torch.tensor([[2.], [4.], [6.]])\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e112218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linea = torch.nn.Linear(1, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linea(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7fab009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel()\n",
    "criterion = torch.nn.MSELoss(reduce='meam')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9960c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.900431156158447\n",
      "10 0.5295999646186829\n",
      "20 0.11281478404998779\n",
      "30 0.07255915552377701\n",
      "40 0.06818318367004395\n",
      "50 0.06724913418292999\n",
      "60 0.06666713207960129\n",
      "70 0.06614184379577637\n",
      "80 0.0656440481543541\n",
      "90 0.06516989320516586\n",
      "100 0.06471806019544601\n",
      "110 0.0642874464392662\n",
      "120 0.06387706845998764\n",
      "130 0.0634860023856163\n",
      "140 0.06311329454183578\n",
      "150 0.06275811046361923\n",
      "160 0.062419623136520386\n",
      "170 0.06209703907370567\n",
      "180 0.06178960204124451\n",
      "190 0.06149662658572197\n",
      "w =  0.4126071631908417\n",
      "b =  -0.4680022597312927\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print('w = ', model.linea.weight.item())\n",
    "print('b = ', model.linea.bias.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "635c07fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3572],\n",
       "        [2.8329]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.tensor([[2.], [8,]])\n",
    "y_test = model(x_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7aac6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[0], [0], [1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfb6dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "89b095ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9e4e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9505650401115417\n",
      "10 0.9162644743919373\n",
      "20 0.8868486881256104\n",
      "30 0.8618752360343933\n",
      "40 0.8408457636833191\n",
      "50 0.8232410550117493\n",
      "60 0.8085508346557617\n",
      "70 0.7962965965270996\n",
      "80 0.786047637462616\n",
      "90 0.7774274945259094\n",
      "100 0.77011638879776\n",
      "110 0.763847827911377\n",
      "120 0.7584037184715271\n",
      "130 0.7536080479621887\n",
      "140 0.7493205070495605\n",
      "150 0.7454301714897156\n",
      "160 0.7418498992919922\n",
      "170 0.7385117411613464\n",
      "180 0.7353631854057312\n",
      "190 0.732363224029541\n",
      "200 0.7294804453849792\n",
      "210 0.7266908288002014\n",
      "220 0.7239757180213928\n",
      "230 0.7213211059570312\n",
      "240 0.7187158465385437\n",
      "250 0.7161517143249512\n",
      "260 0.7136223912239075\n",
      "270 0.711122989654541\n",
      "280 0.7086498141288757\n",
      "290 0.7061998248100281\n",
      "300 0.7037710547447205\n",
      "310 0.7013618350028992\n",
      "320 0.6989704966545105\n",
      "330 0.6965964436531067\n",
      "340 0.6942387223243713\n",
      "350 0.6918967366218567\n",
      "360 0.6895699501037598\n",
      "370 0.6872580647468567\n",
      "380 0.6849607825279236\n",
      "390 0.6826775670051575\n",
      "400 0.6804086565971375\n",
      "410 0.6781533360481262\n",
      "420 0.6759119033813477\n",
      "430 0.6736839413642883\n",
      "440 0.671469509601593\n",
      "450 0.6692684292793274\n",
      "460 0.6670804619789124\n",
      "470 0.6649056077003479\n",
      "480 0.662743866443634\n",
      "490 0.6605949401855469\n",
      "500 0.6584588885307312\n",
      "510 0.6563355326652527\n",
      "520 0.6542248725891113\n",
      "530 0.6521267890930176\n",
      "540 0.6500411033630371\n",
      "550 0.6479678750038147\n",
      "560 0.6459071040153503\n",
      "570 0.6438583731651306\n",
      "580 0.6418218612670898\n",
      "590 0.6397975087165833\n",
      "600 0.6377850770950317\n",
      "610 0.6357845067977905\n",
      "620 0.6337957978248596\n",
      "630 0.6318190097808838\n",
      "640 0.6298537850379944\n",
      "650 0.6279001832008362\n",
      "660 0.6259580254554749\n",
      "670 0.6240274906158447\n",
      "680 0.6221082806587219\n",
      "690 0.6202003359794617\n",
      "700 0.6183037161827087\n",
      "710 0.616418182849884\n",
      "720 0.6145436763763428\n",
      "730 0.612680196762085\n",
      "740 0.610827624797821\n",
      "750 0.608985960483551\n",
      "760 0.6071550250053406\n",
      "770 0.6053349375724792\n",
      "780 0.6035253405570984\n",
      "790 0.6017263531684875\n",
      "800 0.5999378561973572\n",
      "810 0.5981598496437073\n",
      "820 0.5963919758796692\n",
      "830 0.5946345925331116\n",
      "840 0.592887282371521\n",
      "850 0.591150164604187\n",
      "860 0.5894231796264648\n",
      "870 0.5877060890197754\n",
      "880 0.5859990119934082\n",
      "890 0.584301769733429\n",
      "900 0.5826143622398376\n",
      "910 0.5809366703033447\n",
      "920 0.5792686343193054\n",
      "930 0.577610194683075\n",
      "940 0.575961172580719\n",
      "950 0.5743216872215271\n",
      "960 0.5726916790008545\n",
      "970 0.5710709095001221\n",
      "980 0.5694593787193298\n",
      "990 0.5678571462631226\n",
      "w =  0.31733742356300354\n",
      "b =  -0.8435739278793335\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a32cfdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3714],\n",
       "        [0.4480],\n",
       "        [0.5271]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099e402",
   "metadata": {},
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff951006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b73014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>year</th>\n",
       "      <th>num</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>75</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>76</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>78</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>83</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  year  num  label\n",
       "0     30    64    1      1\n",
       "1     30    62    3      1\n",
       "2     30    65    0      1\n",
       "3     31    59    2      1\n",
       "4     31    65    4      1\n",
       "..   ...   ...  ...    ...\n",
       "301   75    62    1      1\n",
       "302   76    67    0      1\n",
       "303   77    65    3      1\n",
       "304   78    65    1      2\n",
       "305   83    58    2      2\n",
       "\n",
       "[306 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.read_csv('C:\\\\Users\\\\84910\\\\py\\\\data\\\\haberman.data',  names=['age', 'year', 'num', 'label'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5caf464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Haberman_dataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        df = pd.read_csv(filepath,  names=['age', 'year', 'num', 'label'])\n",
    "        self.len = df.shape[0]\n",
    "        self.x_data = torch.tensor(df.iloc[:,0:3].values, dtype=torch.float32)\n",
    "        self.y_data = torch.tensor(df.iloc[:,[-1]].values - 1, dtype=torch.float32)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "dataset = Haberman_dataset('C:\\\\Users\\\\84910\\\\py\\\\data\\\\haberman.data')\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0f7d25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(3, 16)\n",
    "        self.linear2 = torch.nn.Linear(16, 1)\n",
    "        self.linear3 = torch.nn.Linear(6, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1a01ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(116.3839, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1371, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3857, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8409, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5946, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2430, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6089, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9497, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7383, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6285, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6195, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2602, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0975, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9793, grad_fn=<MseLossBackward0>)\n",
      "tensor(15.9105, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1668, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9566, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6165, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5571, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7963, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4760, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3409, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3811, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2332, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6747, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5783, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3521, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7054, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5563, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2396, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6895, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6681, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5744, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5853, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2368, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7094, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7088, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3252, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9451, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2770, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8665, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5543, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4707, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7135, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3248, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1347, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3940, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4584, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1658, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6462, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7626, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9998, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8412, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0428, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0182, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7875, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5268, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1295, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5425, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4397, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7640, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6600, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5996, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1446, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3161, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7614, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3543, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5727, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5862, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6379, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5470, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7653, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7010, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2205, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5993, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6874, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3470, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5326, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6109, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4783, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5705, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0085, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4068, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1628, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3287, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4426, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5486, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5898, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1667, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7231, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4355, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4980, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2669, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2891, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2767, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6215, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6114, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2511, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1189, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5502, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4145, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4107, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4378, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1440, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3462, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4234, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2192, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3372, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4669, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4822, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2426, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2995, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2855, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1332, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4639, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3151, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6191, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3498, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4104, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2668, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2977, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3244, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4301, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2260, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4192, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5351, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3434, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3460, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3554, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1522, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5485, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3518, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2056, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3697, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4344, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3857, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5599, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3239, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2115, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5281, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1263, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4092, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3488, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1757, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1170, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3360, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3460, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2901, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1808, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1992, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2299, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2909, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3403, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3330, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1867, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2292, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2463, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2912, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2291, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3629, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1519, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1404, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1834, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1814, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2268, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3828, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3461, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2259, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1258, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1807, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1337, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5262, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3459, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1411, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1493, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3634, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3524, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2570, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1322, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4336, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3636, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2835, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1437, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1433, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3369, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1476, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1481, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4201, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3534, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2599, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3077, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1770, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2280, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3303, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2751, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2350, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2436, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1085, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1681, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6173, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1873, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2242, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1992, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3303, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1495, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1460, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2429, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1181, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1316, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2445, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3081, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3044, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3059, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3852, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1727, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1735, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3167, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2471, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1977, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3230, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2060, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1450, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2748, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3440, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2589, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2231, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2581, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1444, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1339, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1303, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2968, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1654, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1248, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3549, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3311, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2515, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2637, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2229, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3215, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2292, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1162, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1458, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2977, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1402, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3124, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1995, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2314, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2794, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3552, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2347, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2206, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1691, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2280, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1492, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2455, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2173, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3579, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1496, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3518, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1281, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3460, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2791, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3135, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2763, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2556, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1906, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2253, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2340, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1820, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2767, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1967, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1809, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3249, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2623, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3592, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1855, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2150, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3357, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2324, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2440, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2251, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3279, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1633, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2682, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2230, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2245, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3675, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2577, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1687, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2963, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2509, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3076, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2104, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1950, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3588, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1510, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3165, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1855, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1956, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2089, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3923, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1747, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1793, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2992, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2454, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1902, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1399, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2344, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1360, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2146, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1695, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3105, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2956, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1673, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2258, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2445, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2264, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4191, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1508, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2821, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2426, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1347, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2510, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2091, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1504, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1568, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3070, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3136, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1639, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2041, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1440, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1993, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1981, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2538, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2328, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2461, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1518, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1394, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2323, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1410, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1760, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1457, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2419, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3275, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2984, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1608, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1633, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2295, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1605, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4627, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2116, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1966, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4379, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1278, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1432, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3456, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1662, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2473, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2821, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2874, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2159, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2652, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0282, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3393, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2752, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1976, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3376, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1338, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2666, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2076, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3239, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1549, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2797, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1371, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2799, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1444, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1490, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1304, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3189, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1444, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1509, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1302, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1461, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2232, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1376, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3163, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1602, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1555, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2625, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1820, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3519, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1193, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2363, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2082, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1926, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1540, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2651, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2890, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1210, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0677, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3087, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2195, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4527, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1880, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3172, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2488, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1362, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1652, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1429, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3556, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1596, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1395, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2361, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1994, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "ls = []\n",
    "for epoch in range(20):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(loss)\n",
    "        ls.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "336ac84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e9fb39e4f0>]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ea2cf627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([43., 58., 52.]), tensor([1.]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "48f2c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3660], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model(dataset[97][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4a28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
